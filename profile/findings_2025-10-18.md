# Profiling Findings — Phase 0 (2025-10-18)

Summary
-------
We ran a release-mode buffered 4-worker (and related) sweep and captured a 30s macOS sampling trace for a 4-worker buffered run. The harness now emits p50/p95/p99 percentiles for TPS, frame flush, and manifest latency and we collected both buffered and durable (fsync-every-frame) measurements.

Key numbers (representative)
- Buffered, release, tuned WAL: 4 workers avg TPS ≈ 35.6k (p50 37.9k, p95 43.6k, p99 44.1k)
- Buffered, 1 worker avg TPS ≈ 244.4k (p99 < 0.25 ms flush)
- Durable (fsync every frame/manifest): 4 workers avg TPS ≈ 18.7k, flush p99 ≈ 23 ms, manifest p99 ≈ 20 ms
- Tail behavior: buffered 8-worker p99 flush and manifest latencies spike (manifest p99 ≈ 1s in one run), indicating occasional serialized slow I/O or global stalls.

What the sample contains
- `profile/runtime_metrics_4w_sample.txt` — macOS `sample` output for a 4-worker buffered run (30s sampling, 1ms interval). The text is symbolized; the top-level executable is `runtime_metrics`. The sample confirms the program is CPU-bound with intermittent stalls in I/O-related code paths (as inferred from correlated latency spikes in the metrics JSON collected during the sampled run).

Observed/likely hotspots (hypotheses)
1. WAL/sled write mutexes — shared WAL appends or synchronized sled tree operations can serialize multiple workers. Evidence: steep throughput cliff at 2 workers and partial recovery only at higher worker counts.
2. Manifest sealing/serialization path — building manifests and proposing them may include global state updates (epoch anchoring) that are not per-core parallelizable.
3. Kernel/fsync stalls on durable runs — durable runs show multi-ms flush times and p99s in tens of milliseconds, consistent with synchronous disk commits.
4. Memory allocator / cache thrash — at higher worker counts we see p99 spikes that could be caused by cache misses or allocator contention.

Immediate next actions (short-term experiments)
- Produce a flamegraph (Linux perf or macOS Instruments) focused on the 4-worker buffered run. If possible, run `perf` on a Linux runner or use Instruments' recording to get a full stack-weighted breakdown.
- Add lightweight counters around WAL operations (per-worker append counts, lock hold times) in the storage wrapper to measure contention without a full code rewrite.
- Run the 2–8 worker buffered sweep under the profiler to correlate throughput drops with hotspots.

Medium-term prototypes to validate mitigations
- Lock-free WAL ingestion: per-thread append buffers + a background single-writer aggregator that writes WAL records in batch. This reduces per-op mutex contention and amortizes syscalls.
- Per-core manifest sealing: allow workers to produce per-core manifests and then merge/compact them before proposing, reducing global critical sections.
- Async flush queue: a dedicated background flusher that serializes fsyncs but decouples application latency from durability (application confirms frame persisted after scheduling fdatasync; strict durability still possible via ordering semantics).

Metrics and validation plan
- Implement an experiment branch with a simple per-thread WAL buffer and compare the 1–8 worker buffered run before/after. Capture p50/p95/p99 and tail latencies.
- Add counters and log histograms for WAL lock wait times and flush duration to correlate code hotspots to observed tail latency.
- If the prototype shows meaningful improvement (≥2× multi-worker throughput lift or significant tail reduction), iterate on correctness (durability/order) and integrate into `Storage` with feature-flag gating.

Artifacts
- `profile/runtime_metrics_4w_sample.txt` (collected)
- `docs/roadmap/profiling_notes.md` (instructions)

Notes
- The sample text is symbolized but high-volume collapsed stacks were truncated in the sample output. Use perf/flamegraph for finer call-weight breakdowns.

Proposed immediate owner/ETA
- Owner: performance lead (assignable)
- ETA: start flamegraph collection this week; initial per-thread WAL prototype + microbench in 1–2 weeks.
